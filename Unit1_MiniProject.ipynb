{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKkq_Ora6F-r"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUWX5Qq86I7D",
        "outputId": "261a1583-f0d8-488c-8e45-d92998361c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "e8c0a961e6014518948426b703ea3893",
            "a600aee9e128452491d1524ae24e29fd",
            "a037af8e646445e8bf896df08cef5804",
            "83dc2c725bc345539791a6bd7d1fe444",
            "50def9f410734c46bc8e3eea0c8ccaf9",
            "59caa44a66ee4a509d5637e648472685",
            "20d21953612b4b8a86a0177d83678883",
            "2f309e3c8d7144c897690b0c481ca213",
            "24ea6a1863734a57baf5d0d0daefb651",
            "43559b40a1244defa35a94ad08549de2",
            "28fa49787d394458adc216b5721e2cbb",
            "c0dbd26e5d6b45ac98da7aa37551e7aa",
            "19e6c67b330149b6ad941da2b7457018",
            "d7021944c97e4c3c8421f57002b2d408",
            "b74b7f7f2c264b35bc7116778fd3832d",
            "7df3195de5c1447b93bf56a92b450bb8",
            "c7c2e167fa8a40d28fed65bdf24f5cbe",
            "d2af2674dd8e4480ac7192d2ae674a15",
            "c051cb344d244ba1b064e38c1804d399",
            "4daad6d9bc5f4588b6f436bdc1c0692b",
            "8dc6983d518f42598d616b7e1674bba3",
            "0340d920206f4cf79f66c368c832e28b",
            "65da5cb5e8e94f7b954ad9f0a5fea255",
            "b04004cfa1f44ce8abcf180e22a8a8d2",
            "d85a35dae3da466a831c4dca573d20ea",
            "366aa0f96025436db89cff77ac2df95c",
            "bf924fabb2984406b51f9f58b83878e8",
            "d526ccb9041a44d3974411a23fa2705a",
            "a523c770a51d40bcb28c8ec623f09c22",
            "9c87e5d33ec0450f9b53eb1df248136f",
            "3fafde9699b640feb2f52a413ad0eef3",
            "65d15a5a102a437da504268edc239790",
            "ff5ee7a84c054ffdbce5626b2ec7830b",
            "e2aac40b782044bc9eb2350e41113531",
            "34119d8eef3c474b9fba04e88778a192",
            "e97037518ab244b087f7a763c3f82029",
            "602180a161454df3ba2eb510b3e491b8",
            "c1900de791634150af6635c15a64346c",
            "1c1fb870a8f3413a9ff8d11da4782836",
            "69e62ffc146243aead3bc953144cbaca",
            "b6a984d526b24482944cc01006af13af",
            "a62126547b654ce39cb062a147908bec",
            "e101af1d6d8c4aa9ae4a0a9b53e7f948",
            "01d3177d93734890aa410c19e52819bf",
            "27e612dc73f04b739a8af52cab669259",
            "e1a269216b054a96b91a684c51e32b62",
            "ce9d39b1c0d24f2d85bc29924aa69b3e",
            "d8b7ea1055c041709454d313eabb93b7",
            "3dc1ec93e5444e3bb5448590072ebf3b",
            "6a040a0d5b9c4c73b9f0a4f61d469fc7",
            "7a86d8d36ac840b9a02ae042ce76ba8a",
            "20f6b6429c2348ff8aa9e12d9c286d7c",
            "515868dd96f04417b9884b0b4ae4f6f9",
            "0eeabda3a62849529352c20e464a1519",
            "20a9037a7b0c4fc1974274503435dbc9",
            "9485105da6534e85809d1f7f8e7f7523",
            "89e1c60c0e974a82b8b2fb738b9e710f",
            "789520bedce040248bcc56cb25b93143",
            "f53e47535937482cbfe8973f9687e89d",
            "6c0b89025c5c45018f9fb9f14f7de22e",
            "1a85ffd70bf646f9a075eb5c3e920dcb",
            "6173d676e9884c90822d5ffb2bff43c3",
            "7f082bf021804ab1954e7b0607ff4ab4",
            "c7cf254e974847fdb75afbb6a9014da6",
            "3881c8513fba4fb2979fbfadc7fff961",
            "8a3f19ef398942538e2da217e9061f78"
          ]
        },
        "id": "EE-fun3b6PNB",
        "outputId": "2fdcf006-3513-4f2f-9381-5d44855d9eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8c0a961e6014518948426b703ea3893"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0dbd26e5d6b45ac98da7aa37551e7aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65da5cb5e8e94f7b954ad9f0a5fea255"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2aac40b782044bc9eb2350e41113531"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e612dc73f04b739a8af52cab669259"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9485105da6534e85809d1f7f8e7f7523"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_text = \"\"\"\n",
        "Generative Artificial Intelligence is transforming industries by enabling machines to generate human-like text, images, and code.\n",
        "However, the technology also poses risks such as misinformation, bias, job displacement, and deepfake creation.\n",
        "Governments and researchers are working to create ethical guidelines and safety frameworks for AI systems.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cgZCJH2W6dlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(news_text, max_length=60, min_length=20, do_sample=False)\n",
        "print(\"TL;DR Summary:\")\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEYyGhYk6tmN",
        "outputId": "5cdc3a58-7ded-495f-cf0a-99b97b8e60e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TL;DR Summary:\n",
            "Governments and researchers are working to create ethical guidelines and safety frameworks for AI systems. The technology also poses risks such as misinformation, bias, job displacement, and deepfake creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this project, a TL;DR News Summarizer was implemented using the Hugging Face `pipeline(\"summarization\")` with the `facebook/bart-large-cnn` model. The system takes a long news article as input and generates a concise summary, demonstrating the capability of transformer-based encoder\u2013decoder architectures for abstractive summarization.\n",
        "\n",
        "BART (Bidirectional and Auto-Regressive Transformers) works by encoding the input text into a contextual representation and then decoding it to generate a shorter summary. Since the model is pretrained on large-scale news datasets (CNN/DailyMail), it can effectively capture key points and produce human-like summaries without additional training.\n",
        "\n",
        "This project highlights how pretrained transformer models and Hugging Face pipelines can be used to build real-world NLP applications with minimal effort. It also demonstrates the importance of selecting the correct model architecture for a specific task, as encoder\u2013decoder models are particularly well-suited for text generation and summarization tasks.\n",
        "\n",
        "Overall, this project demonstrates a practical application of Generative AI and NLP pipelines using pretrained transformer models.\n"
      ],
      "metadata": {
        "id": "1F-0xlWT7Uhs"
      }
    }
  ]
}